import networkx as nx
import os
import re 

from load_conn_graphs import *
from math import log, floor

LOG_DIR = os.path.join('/', 'mnt', 'raid0_24TB', 'datasets', 'pico', 'bro')
PICKLE_GRAPH = 'agg_conn_graph.pkl'

MAX_PKTS = floor(log(240000)) + 1
PKT_OFFSET = 0 # So when 0 pkts sent, mark that in idx 0

MAX_DUR = floor(log(3000)) + 1
MIN_DUR = 1e-06 
DUR_OFFSET = -1*floor(log(MIN_DUR))


''' Builds a graph from the pickle file generated by a call to the previous method.
'''
def load_graph_from_file():
	G = nx.read_gpickle(PICKLE_GRAPH)
	return G


''' Aggregates using edgecentric method. Add method for load_conn_graphs.load_edges 
'''
def aggregate(G, streamer):
	edge = next(streamer)
	
	# Have to format out the timestamp to just be a float
	d = dt.datetime.strptime(edge['ts'], "%Y-%m-%dT%H:%M:%S.%fZ")

	# Initialize vectors
	if (edge['id.orig_h'], edge['id.resp_h']) not in G.edges():
		oIsLocal = edge['local_orig']
		rIsLocal = edge['local_resp']
		G.add_node(edge['id.orig_h'], isLocal=oIsLocal)	
		G.add_node(edge['id.resp_h'], isLocal=rIsLocal)

		proto_vec = [0.0] * len(PROTO_DICT)
		conn_vec = [0.0] * len(CONN_DICT)
		orig_pkt_vec = [0.0] * (MAX_PKTS + PKT_OFFSET)
		resp_pkt_vec = [0.0] * (MAX_PKTS + PKT_OFFSET)
		dur_vec = [0.0] * (MAX_DUR + DUR_OFFSET)
	
		# Keep track of system port traffic
		orig_p_vec = [0.0] * 1024
		resp_p_vec = [0.0] * 1024

		# Keep track of time between contact (if it exists)
		td_vec = [0.0] * 21 # Time delta 
		ts_vec = [0.0] * 24 # Hour of occurence
		last_ts = None

	# Or get current vectors
	else:
		old_e = G.edges()[edge['id.orig_h'], edge['id.resp_h']]

		proto_vec = old_e['proto_vec']
		conn_vec = old_e['conn_vec']
		dur_vec = old_e['dur_vec']
		orig_pkt_vec = old_e['orig_pkt_vec']
		resp_pkt_vec = old_e['resp_pkt_vec']
		orig_p_vec = old_e['orig_p_vec']
		resp_p_vec = old_e['resp_p_vec']
		ts_vec = old_e['ts_vec']
		td_vec = old_e['td_vec']
		last_ts = old_e['last_ts']

	# Not guarenteed to be in the data	
	if 'duration' in edge:
		# I don't think duration can be 0 but just in case
		if edge['duration'] != 0:	
			dur_vec[floor(log(edge['duration'])) + DUR_OFFSET] += 1

	# Update vectors
	conn_vec[CONN_DICT[edge['conn_state']]] += 1
	proto_vec[PROTO_DICT[edge['proto']]] += 1
	ts_vec[d.hour] += 1

	# Only keep track of system ports
	if edge['id.orig_p'] < 1024:
		orig_p_vec[edge['id.orig_p']] += 1
	if edge['id.resp_p'] < 1024:	
		resp_p_vec[edge['id.resp_p']] += 1

	# Avoid log(0) errors	
	if edge['orig_pkts'] != 0:
		orig_pkt_vec[PKT_OFFSET + floor(log(edge['orig_pkts']))] += 1
	else:
		orig_pkt_vec[PKT_OFFSET] += 1
	
	if edge['resp_pkts'] != 0:
		resp_pkt_vec[PKT_OFFSET + floor(log(edge['resp_pkts']))] += 1
	else:
		resp_pkt_vec[PKT_OFFSET] += 1
	
	# Only care about ts delta
	if last_ts != None:
		idx = 0 if d.timestamp()-last_ts == 0 else floor(log(abs(d.timestamp()-last_ts)))
		td_vec[idx] += 1

	# Finally, replace edge with aggregated version
	G.add_edge(	
		edge['id.orig_h'], edge['id.resp_h'],
		orig_p_vec=orig_p_vec, 
		resp_p_vec=resp_p_vec,
		proto_vec=proto_vec,
		orig_pkt_vec=orig_pkt_vec,
		resp_pkt_vec=resp_pkt_vec,
		dur_vec=dur_vec,
		conn_vec=conn_vec, 
		ts_vec=ts_vec, 
		td_vec=td_vec,
		last_ts=d.timestamp()
	)


''' Turns each occurence array into a PMF by dividing each value by the sum of the array
'''
def normalize(G):
	for e in G.edges.data():
		d = e[2]
		
		d.pop('last_ts')
		for k,v in d.items():
			if sum(v) == 0:
				continue

			d[k] = [i/sum(v) for i in v]


''' Only needs to be run once, or if graph structure changes. Saves the resultant graph
	in a pickle file for ease of opening next time. 
'''
def load_all_graphs():
	G = nx.DiGraph()
	
	r = re.compile(r"conn\..+")
	for folder in os.listdir(LOG_DIR):
		for fname in filter(r.match, os.listdir(os.path.join(LOG_DIR, folder))):
			print('Loading ' + fname + '...')

			load_edges(
				os.path.join(LOG_DIR, folder, fname), 
				load_all=True, 
				append_to=G,
				add_func=aggregate		
			)

	print('Normalizing...')
	#normalize(G)
	nx.write_gpickle(G, PICKLE_GRAPH)
	return G
